- [Volume rendering + 3D implicit surface](#volume-rendering--3d-implicit-surface)
- [Showcase](#showcase)
- [What?](#what)
    - [previous: surface rendering; now: volume rendering](#previous-surface-rendering-now-volume-rendering)
    - [previous: NeRF's volume density; now: implicit surface](#previous-nerfs-volume-density-now-implicit-surface)
  - [What's different of the implemented papers:](#whats-different-of-the-implemented-papers)
  - [Future](#future)
- [Results and trained models](#results-and-trained-models)
- [NOTES](#notes)
- [USAGE](#usage)
- [TODO](#todo)
- [CITATION](#citation)
- [Contact](#contact)
- [ğŸ‰ğŸ‰ğŸ‰ We are hiring!](#we-are-hiring)

## Volume rendering + 3D implicit surface = Neural 3D Reconstruction

Multi-view 3D reconstruction using neural rendering. 

### This repository holds :warning:unofficial:warning: pytorch implementations of: 

- **Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction**
    - [[preprint]](https://arxiv.org/abs/2104.10078) 

-  **NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction**
    - [[preprint]](https://arxiv.org/abs/2106.10689) [[official implementation]](https://github.com/Totoro97/NeuS)

- [VolSDF] **Volume rendering of neural implicit surfaces**
    - [[preprint]](https://arxiv.org/abs/2106.12052)

and more...

## Showcase

Trained with [VolSDF](https://arxiv.org/abs/2106.12052)@200k, with [NeRF++](https://github.com/Kai-46/nerfplusplus) as background.

- Above: :rocket: **<u>volume rendering</u>** of the scene (**<u>novel view synthesis</u>**)

- Below: extracted mesh from the learned implicit shape

| ![volsdf_nerf++_blended_norm_5c0d13_rgb&mesh_576x768_450_archimedean_spiral_400](media/volsdf_nerf++_blended_norm_5c0d13_rgb&mesh_576x768_450_archimedean_spiral_256.gif) |
| :----------------------------------------------------------: |
| full-res video: (35 MiB, 15s@576x768@30fps) [[click here]](https://longtimenohack.com/hosted/nerf-surface/volsdf_nerf%2B%2B_blended_norm_5c0d13_rgb%26mesh_576x768_450_archimedean_spiral.mp4) |



Trained with [NeuS](https://arxiv.org/abs/2106.10689) @300k, with [NeRF++](https://github.com/Kai-46/nerfplusplus) as background.

- Above: :rocket: **<u>volume rendering</u>** of the scene (**<u>novel view synthesis</u>**)
- Middle: extracted normals from the learned implicit shape ($\nabla_{\mathbf{x}} s$)
- Below: extracted mesh from the learned implicit shape

| ![neus_55_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256](media/DTU/neus/neus_55_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256.gif) | ![neus_37_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256](media/DTU/neus/neus_37_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256.gif) | ![neus_65_nomask_new_rgb&normal&mesh_360x400_60_small_circle_256](media/DTU/neus/neus_65_nomask_new_rgb&normal&mesh_360x400_60_small_circle_256.gif) |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![neus_97_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256](media/DTU/neus/neus_97_nomask_new_rgb&normal&mesh_300x400_60_small_circle_256.gif) | ![neus_105_nomask_new_rgb&normal&mesh_390x400_60_small_circle_256](media/DTU/neus/neus_105_nomask_new_rgb&normal&mesh_390x400_60_small_circle_256.gif) | ![neus_24_nomask_new_rgb&normal&mesh_360x400_60_small_circle_256](media/DTU/neus/neus_24_nomask_new_rgb&normal&mesh_360x400_60_small_circle_256.gif) |



## What?

The overall topic of the implemented papers is multi-view surface and appearance reconstruction **from pure posed images**. 

- studying and bridging between [[DeepSDF](https://arxiv.org/abs/1901.05103)/[OccupancyNet](https://arxiv.org/abs/1812.03828)]-like implicit 3D surfaces and volume rendering ([NeRF](https://www.matthewtancik.com/nerf)). 
- framework:

![framework](media/framework.png)

| What's known / Ground Truth / Supervision                    | What's learned                      |
| ------------------------------------------------------------ | ----------------------------------- |
| **ONLY** Multi-view **posed RGB images**. (no mask, no GT mesh, nothing.) | 3D surface / shape<br>3D appearance |

#### previous: surface rendering; now: volume rendering

From one perspective, the implemented papers introduce volume rendering to 3D implicit surfaces to differentiably render views and reconstructing scenes using photometric reconstruction loss. 

| Rendering methods in previous surface reconstruction approach | Rendering method in this repo (when training) |
| ------------------------------------------------------------ | --------------------------------------------- |
| Surface rendering                                            | Volume rendering                              |

The benefit of using volume rendering is that it diffuses gradients widely in space, and can efficiently learns a roughly correct shape at the very early beginning of training without mask supervision, avoiding bad local minimas when learning shapes, which is often encountered when using surface rendering even with mask supervision.

| config:<br> [[click me]](configs/volsdf_nerfpp_blended.yaml) | @0 iters                        | @3k iters<br>@16 mins           | @10k iters<br> @1 hours         | @200k iters<br>@ 18.5 hours       |
| ------------------------------------------------------------ | ------------------------------- | ------------------------------- | ------------------------------- | --------------------------------- |
| Extracted mesh from learned shape                            | ![mesh_0k](media/mesh_0k.png)   | ![mesh_3k](media/mesh_3k.png)   | ![mesh_10k](media/mesh_10k.png) | ![mesh_200k](media/mesh_200k.png) |
| Rendered view from learned appearance                        | ![00000000](media/00000000.png) | ![00003000](media/00003000.png) | ![00010000](media/00010000.png) | ![00200000](media/00200000.png)   |



#### previous: NeRF's volume density; now: implicit surface

From another perspective, they change the original NeRF's shape representation (volume density $\sigma$) to a 3D implicit surface model, whose iso-surface is defined to represent spatial surfaces.

| Shape representation in NeRF | Shape representation in this repo                            |
| ---------------------------- | ------------------------------------------------------------ |
| Volume density               | [Occupancy net](https://arxiv.org/abs/1812.03828) (UNISURF) <br>[DeepSDF](https://arxiv.org/abs/1901.05103) (VolSDF/NeuS) |

The biggest disadvantage of NeRF's shape representation is that it considers objects volume clouds, which actually does not guarantees an exact surface, since there is no constraint on the learned density. 

Representing shapes with implicit surfaces can force the volume density to be associated with a exact surface. 

What's more, the association (or, the mapping function that maps implicit surface value to volume density) can be **controlled** either manually or by learn-able parameters, allowing the shape representation to be more surface-like or more volume-like, meeting different needs of different training stages.

| <img src="media/sdf2sigma.gif" alt="sdf2sigma" style="zoom:50%;" /> |
| :----------------------------------------------------------: |
| Demonstration of controllable mappings from sdf value to volume density value. @VolSDF |

Hence, the training scheme can be roughly divided as follows (not discrete stages, continuously progressing instead):

- at the earlier stage of learning shapes, the shape representation is more volume-like, taking into account of the more neighboring points along the ray for rendering colors. The network fast learns a roughly correct shape and appearance.
- while in the later stage, the shape representation is more surface-like, almost only taking into account the exact intersected points of the ray with the surface. The network slowly learns the fine thin structures of shapes and fine details of appearance.

You can see that as the controlling parameter let narrower and narrower neighboring points being considered during volume rendering, the rendered results are almost equal to surface rendering. This is proved in [UNISURF](https://arxiv.org/abs/2104.10078), and also proved with results showed in the section [[docs/usage.md#use surface rendering instead of volume rendering]](docs/usage.md#pushpin-use-surface-rendering-instead-of-volume-rendering).

![limit](media/limit.png)



### What's different of the implemented papers:

- how to map a implicit surface value to a volume density representation, or how to (accurately) calculate volume rendering's opacity with such exact surface representation.
- how to efficiently sample points on camera rays taking advantage of the exact surface
- You can find out more on my [[personal notes]](https://longtimenohack.com/posts/nerf/nerf_on_surface/) (In Chinese only).



### Future

Currently, the biggest problem of methods contained in this repo is that the view-dependent reflection effect is **baked** into the object's surface, similar with [IDR](https://github.com/lioryariv/idr), [NeRF](https://www.matthewtancik.com/nerf) and so on. In other words, if you place the learned object into a new scene with different ambient light settings, the rendering process will have no consideration of the new scene's light condition, and keeps the ambient light's reflection of the old trained scene with it.

However, as the combination of implicit surface with NeRF has come true, ambient light and material decomposition can be easier for NeRF-based frameworks, since now shapes are represented by the underlying neural surface instead of volume densities.



## Results and trained models

The trained models are stored in [[GoogleDrive]](https://drive.google.com/drive/folders/1B7y-nMFO9noVI0byU34yPTRtqqzMdMIQ?usp=sharing) / [[Baidu, code: `reco`]](https://pan.baidu.com/s/10g1IWwrGrpE--VJ5XLuRFw).

For more visualization of the my trained results, see [[docs/trained_models_results.md]](docs/trained_models_results.md).


## USAGE

See [[docs/usage.md]](docs/usage.md) for detailed usage documentation.


## NOTES

- [[docs/neus.md]](docs/neus.md) Notes on the `unbiased` property of NeuS.
- [[docs/volsdf.md]](docs/volsdf.md) Notes on the `error bound` and `up sampline algorithm` of VolSDF.

- [[click here]](https://longtimenohack.com/posts/nerf/nerf_on_surface/) (in Chinese) My personal notes æˆ‘çš„ä¸ªäººç¬”è®° 


## TODO

- NeuS
  - [x] Compare with NeuS official repo. 
  - [x] Fix performance bug (camera inside surface after training) on some of the DTU instances.

- VolSDF
  - [ ] improve VolSDF's sampling performance
  - [ ] release more results
  
- UNISURF
  - [x] Fix performance bug (huge artifact after training) on some of the DTU instances.

- general
  - [ ] train camera
  - [x] cluster training configs
  - [x] DDP support
  - [x] refine GPU usage and try to allow for at least 2080 Ti.
  - [x] surface rendering option.
  - [ ] eval script for RGB
  - [ ] eval script for mesh CD



## CITATION

- UNISURF

```python
@article{oechsle2021unisurf,
  title={Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction},
  author={Oechsle, Michael and Peng, Songyou and Geiger, Andreas},
  journal={arXiv preprint arXiv:2104.10078},
  year={2021}
}
```

- NeuS

```python
@article{wang2021neus,
  title={NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
  author={Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  journal={arXiv preprint arXiv:2106.10689},
  year={2021}
}
```

- VolSDF

```python
@article{yariv2021volume,
  title={Volume Rendering of Neural Implicit Surfaces},
  author={Yariv, Lior and Gu, Jiatao and Kasten, Yoni and Lipman, Yaron},
  journal={arXiv preprint arXiv:2106.12052},
  year={2021}
}
```

- NeRF++

```python
@article{kaizhang2020,
    author    = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
    title     = {NeRF++: Analyzing and Improving Neural Radiance Fields},
    journal   = {arXiv:2010.07492},
    year      = {2020},
}
```

- SIREN

```python
@inproceedings{sitzmann2019siren,
    author = {Sitzmann, Vincent and Martel, Julien N.P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
    title = {Implicit Neural Representations with Periodic Activation Functions},
    booktitle = {Proc. NeurIPS},
    year={2020}
}
```

## Acknowledgement

This repository modifies code or draw inspiration from:

- My another NeRF-- repo: https://github.com/ventusff/improved-nerfmm
- https://github.com/Totoro97/NeuS

- https://github.com/lioryariv/idr

- https://github.com/autonomousvision/differentiable_volumetric_rendering
- https://github.com/yenchenlin/nerf-pytorch
- https://github.com/YoYo000/MVSNet

- https://github.com/LMescheder/GAN_stability
- https://github.com/Kai-46/nerfplusplus



## Contact

If you have any problems, feel free to submit issues or contact Jianfei Guo(éƒ­å»ºé) `guojianfei [at] pjlab.org.cn`. PRs are also very welcome :smiley:.

## We are hiring! 

ğŸ‰ğŸ‰ğŸ‰

On behalf of Intelligent Transportation and Auto Driving Group in Shanghai AI Lab, we are hiring researcher/engineer/full-time intern for Computer Graphics and 3D Rendering Algorithm (base in Shanghai)

ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ™ºæ…§äº¤é€šä¸è‡ªåŠ¨é©¾é©¶å›¢é˜Ÿæ‹›è˜ã€Œå›¾å½¢å­¦ç®—æ³•ç ”ç©¶å‘˜ã€å’Œã€Œ3Dåœºæ™¯ç”Ÿæˆç ”ç©¶å‘˜ã€ã€‚å®ä¹ ã€æ ¡æ‹›ã€ç¤¾æ‹›å‡æœ‰æµ·é‡HCã€‚


### :sparkles: å›¾å½¢å­¦ç®—æ³•ç ”ç©¶å‘˜
#### å²—ä½èŒè´£
1. ç»“åˆè®¡ç®—æœºå›¾å½¢å­¦å’Œæ·±åº¦å­¦ä¹ è¿›è¡Œç ”ç©¶ï¼Œå¹¶æ¢ç´¢ç›¸å…³æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®ä¸Šçš„åº”ç”¨ã€‚
2. ç ”ç©¶åŸºäºç¥ç»æ¸²æŸ“å™¨ã€å¯å¾®æ¸²æŸ“ç­‰æŠ€æœ¯çš„é«˜è´¨é‡ä¸‰ç»´é‡å»ºæŠ€æœ¯ã€‚
3. ç ”ç©¶é¢å‘æ•°å­—å­ªç”Ÿçš„åŸå¸‚åœºæ™¯é‡å»ºæŠ€æœ¯ã€‚
4. åœ¨å›¾å½¢å­¦å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸè·Ÿè¸ªå­¦æœ¯å’Œäº§ä¸šå‰æ²¿æ–¹æ¡ˆï¼Œåœ¨å•ç‚¹ç®—æ³•ä¸Šå®ç°çªç ´ä¸šç•Œã€åŸå‹éªŒè¯ï¼ŒæŒç»­æ„å»ºä¸‰ç»´é‡å»ºé¢†åŸŸçš„æŠ€æœ¯ç«äº‰åŠ›ã€‚
5. è®¾è®¡å®ç°åŸºäºé«˜è´¨é‡ä¸‰ç»´é‡å»ºæ•°æ®åµŒå…¥çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä»¿çœŸå¹³å°ï¼Œæå‡è‡ªåŠ¨é©¾é©¶ä»¿çœŸæ•°æ®åœ¨æ„ŸçŸ¥æ¨¡å‹ä¸Šçš„å¯ç”¨æ€§ã€‚

#### ä»»èŒè¦æ±‚
1. è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ã€ç”µå­å·¥ç¨‹ã€è‡ªåŠ¨åŒ–ã€äººå·¥æ™ºèƒ½ã€åº”ç”¨æ•°å­¦ã€è½¦è¾†å·¥ç¨‹ç­‰ç›¸å…³ä¸“ä¸šï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ã€‚
2. æœ‰æ‰å®çš„å›¾å½¢å­¦åŸºç¡€ï¼Œç†è§£åŸºæœ¬æ¸²æŸ“ç®¡çº¿ï¼Œäº†è§£PBRå…‰ç…§ã€æè´¨æ¨¡å‹ã€‚
3. ç†Ÿæ‚‰æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ çš„åŸºæœ¬ç†è®ºï¼Œç†Ÿæ‚‰ç¼–ç¨‹/è„šæœ¬è¯­è¨€:C++/C/Pythonï¼Œè‡³å°‘ä½¿ç”¨è¿‡PyTorchã€TensorFlowã€Caffeç­‰æ¡†æ¶ä¸­çš„ä¸€ç§ã€‚
4. åŠ åˆ†é¡¹ï¼šç†Ÿæ‚‰å¹¶æŒæ¡ä¸€ä¸ªæˆ–å¤šä¸ªä¸‹åˆ—æ–¹å‘ï¼šç¥ç»ç½‘ç»œéšå¼ä¸‰ç»´è¡¨ç¤ºï¼ˆå¦‚NeRFï¼‰ï¼Œåœºæ™¯ä¸‰ç»´é‡å»ºï¼Œ3Däººä½“é‡å»ºä¸ç”Ÿæˆ
5. åŠ åˆ†é¡¹ï¼šç†è§£æ¸²æŸ“ç®¡çº¿ä¸­çš„GPUç¼–ç¨‹åŸç†ï¼Œæˆ–å…·æœ‰GPU CUDAå¼€å‘å’Œæ€§èƒ½ä¼˜åŒ–ç»éªŒ
6. åŠ åˆ†é¡¹ï¼šæœ‰Unity, UE4ç­‰æ¸¸æˆå¼•æ“æˆ–Carlaã€AirSimç­‰ä»¿çœŸé¡¹ç›®çš„å¼€å‘ä½¿ç”¨ç»éªŒï¼Œæˆ–ç†Ÿæ‚‰è½¦è¾†ã€è¡Œäººè¿åŠ¨ä»¿çœŸ
7. åŠ åˆ†é¡¹ï¼šæœ‰è‡ªåŠ¨é©¾é©¶ã€è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸç›¸å…³é¡¹ç›®å’Œç ”ç©¶èƒŒæ™¯ã€‚
8. åŠ åˆ†é¡¹ï¼šæœ‰ç›¸å…³é¢†åŸŸé¡¶çº§å­¦æœ¯ä¼šè®®/æœŸåˆŠè®ºæ–‡å‘è¡¨ç»å†ã€‚

### :sparkles: 3Dåœºæ™¯ç”Ÿæˆç ”ç©¶å‘˜

#### å²—ä½èŒè´£
1. ç ”ç©¶åŸºäºæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®ä¸Šçš„åº”ç”¨ã€‚
2. ç ”ç©¶åˆ©ç”¨GAN/VAEç­‰ç”Ÿæˆå­¦ä¹ æŠ€æœ¯é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥åœºæ™¯æ•°æ®ç”Ÿæˆç®—æ³•ã€‚
3. è´Ÿè´£åŸºäºæ·±åº¦å­¦ä¹ ä¸ç”Ÿæˆæ¨¡å‹ç®—æ³•è¿›è¡Œè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥æ•°æ®ä»¿çœŸã€‚
4. è´Ÿè´£è·Ÿè¸ªå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå…³äºç”Ÿæˆæ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºæ…§åŸå¸‚ã€æ•°å­—å­ªç”Ÿç­‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚
5. è®¾è®¡å®ç°åŸºäºç”Ÿæˆæ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä»¿çœŸå¹³å°ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆæ•°æ®æå‡ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ã€‚

#### ä»»èŒè¦æ±‚
1. è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ã€ç”µå­å·¥ç¨‹ã€è‡ªåŠ¨åŒ–ã€äººå·¥æ™ºèƒ½ã€åº”ç”¨æ•°å­¦ã€è½¦è¾†å·¥ç¨‹ç­‰ç›¸å…³ä¸“ä¸šï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ã€‚
2. æœ‰æ‰å®çš„æ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œç†Ÿæ‚‰æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ çš„åŸºæœ¬ç†è®ºï¼Œå¹¶è¦æ±‚è‡³å°‘ç†Ÿç»ƒæŒæ¡PyTorchã€TensorFlowã€Caffeç­‰æ¡†æ¶ä¸­çš„ä¸€ç§ã€‚
3. ä»£ç èƒ½åŠ›å¼ºï¼Œæœ‰è‰¯å¥½çš„C++/Pythonç¼–ç ä¹ æƒ¯ï¼Œèƒ½å¤Ÿå¿«é€Ÿè®¾è®¡å¹¶æ‰§è¡Œå®éªŒã€éªŒè¯æƒ³æ³•ã€å…·å¤‡èƒ½æ”¯æŒå…¨æ ˆå¼€å‘å·¥ä½œçš„å­¦ä¹ èƒ½åŠ›ã€‚
4. åŠ åˆ†é¡¹ï¼šæœ‰è®¡ç®—æœºå›¾å½¢å­¦ã€ç«‹ä½“è§†è§‰ã€ä¸‰ç»´é‡å»ºã€ç¥ç»æ¸²æŸ“ã€å¯å¾®æ¸²æŸ“ç­‰æ–¹å‘ç ”ç©¶ç»å†ã€‚
5. åŠ åˆ†é¡¹ï¼šç†Ÿæ‚‰å¹¶æŒæ¡ä¸€ä¸ªæˆ–å¤šä¸ªä¸‹åˆ—æ–¹å‘ï¼š3Dç”Ÿæˆæ¨¡å‹ï¼Œ3Däººä½“é‡å»ºä¸ç”Ÿæˆï¼Œç¥ç»ç½‘ç»œéšå¼ä¸‰ç»´è¡¨ç¤ºï¼ˆå¦‚NeRFï¼‰
6. åŠ åˆ†é¡¹ï¼šæœ‰è‡ªåŠ¨é©¾é©¶é¢†åŸŸç›¸å…³é¡¹ç›®å’Œç ”ç©¶èƒŒæ™¯ã€‚
7. åŠ åˆ†é¡¹ï¼šåœ¨ç›¸å…³é¢†åŸŸæœ‰é¡¶çº§å­¦æœ¯ä¼šè®®/æœŸåˆŠè®ºæ–‡å‘è¡¨ç»å†ã€‚

----

**å¯¹ä»¥ä¸Šä¸¤ä¸ªå²—ä½æ„Ÿå…´è¶£çš„åŒå­¦è¯·å‘é€ç®€å†åˆ° `shibotian [at] pjlab.org.cn`, `guojianfei [at] pjlab.org.cn`ã€‚æ ‡é¢˜åŠ¡å¿…åŒ…å«ã€Œåº”è˜ã€ä¸¤ä¸ªå­—ï¼Œè°¢è°¢ã€‚**

---

### About Shanghai AI Lab/å…³äºä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤

ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ˜¯ä¸­å›½äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ–°å‹ç§‘ç ”æœºæ„ï¼Œç”±æ±¤æ™“é¸¥ã€å§šæœŸæ™ºã€é™ˆæ°ç­‰å¤šä½ä¸–ç•Œäººå·¥æ™ºèƒ½é¢†åŸŸçŸ¥åå­¦è€…é¢†è¡”å‘èµ·æˆç«‹ï¼Œäº2020å¹´7æœˆåœ¨ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šæ­£å¼æ­ç‰Œã€‚

å®éªŒå®¤ç ”ç©¶å›¢é˜Ÿç”±ä¸€æµç§‘å­¦å®¶å’Œå›¢é˜ŸæŒ‰æ–°æœºåˆ¶ç»„å»ºã€‚å¹¶å¼€å±•æˆ˜ç•¥æ€§ã€åŸåˆ›æ€§ã€å‰ç»æ€§çš„ç§‘å­¦ç ”ç©¶ä¸æŠ€æœ¯æ”»å…³ï¼Œçªç ´äººå·¥æ™ºèƒ½çš„é‡è¦åŸºç¡€ç†è®ºå’Œå…³é”®æ ¸å¿ƒæŠ€æœ¯ï¼Œæ‰“é€ â€œçªç ´å‹ã€å¼•é¢†å‹ã€å¹³å°å‹â€ä¸€ä½“åŒ–çš„å¤§å‹ç»¼åˆæ€§ç ”ç©¶åŸºåœ°ï¼Œæ”¯æ’‘ä¸­å›½äººå·¥æ™ºèƒ½äº§ä¸šå®ç°è·¨è¶Šå¼å‘å±•ï¼Œç›®æ ‡å»ºæˆå›½é™…ä¸€æµçš„äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼Œæˆä¸ºäº«èª‰å…¨çƒçš„äººå·¥æ™ºèƒ½åŸåˆ›ç†è®ºå’ŒæŠ€æœ¯çš„ç­–æºåœ°ã€‚

å®éªŒå®¤å…ˆåä¸ä¸Šæµ·äº¤é€šå¤§å­¦ã€å¤æ—¦å¤§å­¦ã€æµ™æ±Ÿå¤§å­¦ã€ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€åŒæµå¤§å­¦ã€åä¸œå¸ˆèŒƒå¤§å­¦ç­‰çŸ¥åé«˜æ ¡ç­¾è®¢æˆ˜ç•¥åˆä½œåè®®ï¼Œå»ºç«‹ç§‘ç ”äººå‘˜åŒè˜å’ŒèŒç§°äº’è®¤æœºåˆ¶ï¼Œæ±‡èšå›½å†…å›½é™…ä¼˜åŠ¿èµ„æºï¼Œæ¢ç´¢å»ºç«‹åˆ›æ–°å‹çš„è¯„ä»·è€ƒæ ¸åˆ¶åº¦å’Œå…·æœ‰å›½é™…ç«äº‰åŠ›çš„è–ªé…¬ä½“ç³»åŠæ¡ä»¶ä¿éšœã€‚

ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å®˜æ–¹ç½‘ç«™ï¼š[https://www.shlab.org.cn/](https://www.shlab.org.cn/)

æ™ºèƒ½äº¤é€šä¸è‡ªåŠ¨é©¾é©¶ç ”å‘å²—ä½æ‹›è˜ï¼š[https://www.shlab.org.cn/news/5443060](https://www.shlab.org.cn/news/5443060)
